<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on Script Adults</title><link>https://katsuragicsl.github.io/posts/</link><description>Recent content in Posts on Script Adults</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>&lt;a href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank" rel="noopener">CC BY-NC 4.0&lt;/a></copyright><lastBuildDate>Sat, 04 Jun 2022 16:26:45 +0800</lastBuildDate><atom:link href="https://katsuragicsl.github.io/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>Flaws of Maximum Munch in (F)lex</title><link>https://katsuragicsl.github.io/posts/2022/06/flaws-of-maximum-munch-in-flex/</link><pubDate>Sat, 04 Jun 2022 16:26:45 +0800</pubDate><guid>https://katsuragicsl.github.io/posts/2022/06/flaws-of-maximum-munch-in-flex/</guid><description>Introduction Recently I am studying compilers and it reminds me a vulnerability class which is called ReDOS I saw a long time ago. This post will discuss some flaws of lexers, specifically on flex.
Lexer In theory, a compiler contains 5 components:
Lexical analyzer -&amp;gt; identifying tokens (valid &amp;ldquo;words&amp;rdquo; of the source language) Parser -&amp;gt; identifying grammatical structure Semantic analyzer -&amp;gt; identifying semantics (e.g. binding objects to identifiers) Optimizer -&amp;gt; optimizing the programs Code generator -&amp;gt; generate the target language The job of a lexical analyzer, or lexer, scanner, is providing the type and value of a token when fed with input (intended to be code written in source language).</description><content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>Recently I am studying compilers and it reminds me a vulnerability class which is called <a href="https://owasp.org/www-community/attacks/Regular_expression_Denial_of_Service_-_ReDoS">ReDOS</a> I saw a long time ago. This post will discuss some flaws of lexers, specifically on <a href="https://github.com/westes/flex">flex</a>.</p>
<h2 id="lexer">Lexer</h2>
<p>In theory, a compiler contains 5 components:</p>
<ol>
<li><strong>Lexical analyzer</strong> -&gt; identifying tokens (<em>valid &ldquo;words&rdquo; of the source language</em>)</li>
<li><strong>Parser</strong> -&gt; identifying grammatical structure</li>
<li><strong>Semantic analyzer</strong> -&gt; identifying semantics (<em>e.g. binding objects to identifiers</em>)</li>
<li><strong>Optimizer</strong> -&gt; optimizing the programs</li>
<li><strong>Code generator</strong> -&gt; generate the target language</li>
</ol>
<p>The job of a lexical analyzer, or <strong>lexer</strong>, <strong>scanner</strong>, is providing the type and value of a token when fed with input (intended to be code written in source language).</p>
<p>For example, let say we have a source language <strong>L</strong> and all you can code with it is assigning an identifier or expressions to some another identifier by equal sign (of course it is absurd in reality). Then when the lexer is given the following as input:</p>
<p>$$x = 1$$</p>
<p>it should gives <code>&lt;Identifier, &quot;x&quot;&gt; &lt;operator, &quot;=&quot;&gt; &lt;numerics, &quot;1&quot;&gt;</code>, which specifies the type of each token.</p>
<p>At the same time it should be able to catch invalid input such as $$x &gt; 1$$, $$y???2$$.</p>
<p>Also, the lexer needs to deal with ambiguity such as in C++ nested templates</p>
<pre><code>Oh&lt;My&lt;God&gt;&gt;
</code></pre><p>the <code>&gt;&gt;</code> at the ends should be distinguishable from stream operator like which in</p>
<pre><code>std::cin &gt;&gt; something;
</code></pre><p>To achive its goal, lexers make use of regex to match input. Let&rsquo;s use flex as an example.</p>
<h2 id="flex-and-maximum-munch">Flex and Maximum Munch</h2>
<p>In short, flex is a lexer generator for C. It consumes flex code and output the  C implementation of the corresponding lexer.</p>
<p>A flex file has the following format<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>:</p>
<pre><code>%%
regex_1    {action_1}
regex_2    {actin_2}
[...]
</code></pre><p>The lexer it creates will take one char at a time and evaluates it. For example when input &ldquo;aaa&rdquo; is consumed by the lexer generated by the following flex code</p>
<pre><code>%%
aaa    {printf(&quot;aaa\n&quot;);}
b       {printf(&quot;b\n&quot;);}
</code></pre><p>the first &ldquo;a&rdquo; in the input is being evaluated. However, it doesn&rsquo;t match any rules above, so the lexer eats one more char and now it has &ldquo;aa&rdquo;. Again, it doesn&rsquo;t match any rules so we have &ldquo;aaa&rdquo; now. Finally it matches the first rule so the lexer <code>printf(&quot;aaa\n&quot;);</code></p>
<p>What if nothing got matched after reading the whole input?</p>
<p>That&rsquo;s why in reality we need a extra rule to catch anything we don&rsquo;t want (just like the default deny rule at the bottom in firewall)</p>
<pre><code>%%
aaa    {printf(&quot;aaa\n&quot;);}
b       {printf(&quot;b\n&quot;);}
 /*do nothing when hits anything else or newline*/
.|\n    {}
</code></pre><p>So now anything other than &ldquo;aaa&rdquo; and &ldquo;bbb&rdquo; will be ignored.</p>
<p>Wait, why is the lexer not matching <code>.|\n</code> when it is looking at the first &ldquo;a&rdquo;?</p>
<p>Here is the principle which called <strong>Maximum Munch</strong>.<br>
As its name suggests, when substrings <code>s_1</code> and <code>s_2</code> with <code>len(s_1) &gt; len(s_2)</code> of input <code>s</code> matches some rules, we should (and flex will) take <code>s_1</code>.<br>
Hence in the above case, &ldquo;aaa&rdquo; will be matched, instead of matching <code>.|\n</code> three times.</p>
<h2 id="flaws-of-maximum-munch">Flaws of Maximum Munch</h2>
<p>Imagine we need to read the string &ldquo;xxxx&rdquo; as two &ldquo;xx&rdquo;. Also &ldquo;xxx&rdquo; should be read as &ldquo;xxx&rdquo;.</p>
<p>A naive flex code to catch it will be:</p>
<pre><code>%%
xx    {printf(&quot;xx\n&quot;);}
xxx    {printf(&quot;xxx\n&quot;);}
</code></pre><p>but this is not going to work as expected. By maximum munch, &ldquo;xxxx&rdquo; will be read as &ldquo;xxx&rdquo; + &ldquo;x&rdquo; (unknown token).</p>
<p>How about this:</p>
<pre><code>%%
xxxx   {printf(&quot;We got two 'xx' !\n&quot;);}
xxx    {printf(&quot;xxx\n&quot;);}
</code></pre><p>it seems avoided &ldquo;xxx&rdquo; at he first glance but it is actually not what we want. This lexer reads the whole &ldquo;xxxx&rdquo; as <strong>1</strong> token, instead of <strong>2</strong> &ldquo;xx&rdquo; token. Maximum Munch does not work all the time!</p>
<p>Luckily, with the help of <a href="https://westes.github.io/flex/manual/Patterns.html">lookahead</a>, we can solve this problem:</p>
<pre><code>%%
xx    { printf(&quot;xx\n&quot;);}
xx/xx    { printf(&quot;xx,\n&quot;);}
xxx   { printf(&quot;xxx\n&quot;);}
 /* do nothing */
.|\n     {}
</code></pre><p>the <code>/</code> in the second rule helps us to match the whole string (&ldquo;xxxx&rdquo;) but just consume the first two x. So due the maximum munch &ldquo;xxx&rdquo; won&rsquo;t be matched because it is shorter than &ldquo;xxxx&rdquo;. After consuming the first two x, the remaining two x match the first rule, so now we have two &ldquo;xx&rdquo;.</p>
<h2 id="abusing-maximun-munch-to-slow-down-the-lexer">Abusing maximun munch to slow down the lexer</h2>
<p>According to the <a href="https://westes.github.io/flex/manual/Limitations.html#Limitations">manual</a>, the following rules won&rsquo;t be matched properly</p>
<pre><code>%%
zx*/xy    {}
</code></pre><p>because the <code>zx*</code> eats up the x&rsquo;s which should belongs to <code>xy</code>.</p>
<p>Now consider such rules:</p>
<pre><code>%%
x*/xy    {printf(&quot;OK!\n&quot;);}
x           {printf(&quot;just one x&quot;);}
</code></pre><p>when <code>xxxy</code> is provided as input, since the first rule can;t be matched properly, each <code>x</code> before the <code>y</code> will be matched one by one.</p>
<p>Each time the lexer sees &ldquo;x&rdquo;, because it can probably match the first rule, so it will read one more &ldquo;x&rdquo; into the buffer and decide whether it matches, until it has read the whole input string <code>xxxy</code>. But since it can&rsquo;t be properly matched, the second rule is matched instead and now we have <code>xxy</code>. Then the same process repeats.</p>
<p>Note that this process is of $$\Theta(n)$$ where $$n$$ is the number of &ldquo;x&rdquo;. This can slow down the lexer when the input is very large. You can find more vulnerable regexes on <a href="https://owasp.org/www-community/attacks/Regular_expression_Denial_of_Service_-_ReDoS">OWASP</a>.</p>
<p>Lastly, the above rule can be simpler:</p>
<pre><code>%%
x*y    {printf(&quot;OK!\n&quot;);}
x        {printf(&quot;just one x&quot;);}
</code></pre><p>with input &ldquo;xxxxxxxxxxxx[&hellip;]&quot;.</p>
<p>The version with lookahead was just a heuristic for coming up with the idea of how to slow down a lexer.</p>
<h2 id="appendix">Appendix</h2>
<p>It is simple if you want to try flex on linux. For instance on ubuntu, you can write your flex code inside a file with .flex extension, then you type something as input after:</p>
<pre><code>flex test.flex
gcc -ll lex.yy.c
./a.out
</code></pre><section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p>For simplicity, I have pruned off many details of flex syntax and focus on the regex matching part. Anyone interested can refer to the <a href="https://westes.github.io/flex/manual/index.html">flex manual</a>. <a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>
]]></content></item><item><title>Classic API Unhooking</title><link>https://katsuragicsl.github.io/posts/2022/04/classic-api-unhooking/</link><pubDate>Thu, 14 Apr 2022 20:54:39 +0800</pubDate><guid>https://katsuragicsl.github.io/posts/2022/04/classic-api-unhooking/</guid><description>Introduction AV and EDR use API hoooking to monitor API calls of processes. This post will take a brief look on how does a hook look like and the classic solution for malwares to bypass API hooking.
I tested the content of this blog in a windows 10 VM with BitDefender installed.
You can get the sample code here.
API hooking On the Windows VM with BitDefender installed, if you load a PE into x64dbg and look for NtMapViewOfSection, you will see the first instruction is jmp to something.</description><content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>AV and EDR use API hoooking to monitor API calls of processes. This post will take a brief look on how does a hook look like and the classic solution for malwares to bypass API hooking.</p>
<p>I tested the content of this blog in a windows 10 VM with BitDefender installed.</p>
<p>You can get the sample code <a href="https://github.com/KatsuragiCSL/classic-unhooking">here</a>.</p>
<h2 id="api-hooking">API hooking</h2>
<p><img src="/classic-api-unhooking/s1.png" alt=""></p>
<p><img src="/classic-api-unhooking/s2.png" alt=""></p>
<p>On the Windows VM with BitDefender installed, if you load a PE into x64dbg and look for <code>NtMapViewOfSection</code>, you will see the first instruction is jmp to something.<br>
This is NOT how <code>ZwMapViewOfSection</code> looks like originally, but an inline hook on this function by BitDefender.</p>
<p>BitDefender set the hook in order to perform its job before the function runs. In order to evade from BitDefender, you need to evade from this hook. The method discussed in this post is known as classic unhooking.</p>
<h2 id="creating-a-view-of-a-fresh-copy-of-ntdlldll">Creating a view of a fresh copy of ntdll.dll</h2>
<p><img src="/classic-api-unhooking/1.png" alt=""></p>
<p>This is the part responsible for creating a view of the fresh copy of ntdll.dll on disk.</p>
<p>We also need a handle of ntdll.dll which loaded into the current process. It is the one which (some of) its functions are hooked. Let&rsquo;s call it <code>pollutedNtdll</code>.</p>
<h2 id="rewrite-the-text-section">Rewrite the .text section</h2>
<p><img src="/classic-api-unhooking/2.png" alt=""></p>
<p>These few lines are for getting to the section header (and number of sections for iteration). If you are not familiar with PE structure, you could take a look at <a href="https://github.com/corkami/pics/tree/master/binary/pe101">PE101</a> which has nice pictures. You may also want to refer to MSDN pages like <a href="https://docs.microsoft.com/en-us/windows/win32/api/winnt/ns-winnt-image_nt_headers64">this</a>.</p>
<p>We also defined a <code>DWORD</code> variable <code>oldprotect</code> for saving the <a href="https://docs.microsoft.com/en-us/windows/win32/api/memoryapi/nf-memoryapi-virtualprotect">old access protection value</a>.</p>
<p><img src="/classic-api-unhooking/3.png" alt=""></p>
<p>You can get the number of sections in a PE file in the <code>IMAGE_FILE_HEADER</code> structure. We iterate through the sections and find the <code>.text</code> section.<br>
If we get it, we make the <code>.text</code> section of pollutedNtdll writable, by <code>VirtualProtect</code>.</p>
<p><img src="/classic-api-unhooking/4.png" alt=""></p>
<p>Now we can copy the <code>.text</code> section from the fresh copy of <code>ntdll.dll</code> to the one polluted.</p>
<p><img src="/classic-api-unhooking/5.png" alt=""></p>
<p>Finally make sure we recover the access protection.</p>
<h2 id="ntdlldll-unhooked">Ntdll.dll unhooked</h2>
<p>Now we set a breakpoint on an instruction after the unhooking (in this case it is where &ldquo;done!&rdquo; is printed).<br>
Run it and the breakpoint is hit:</p>
<p><img src="/classic-api-unhooking/s3.png" alt=""></p>
<p>Back to the address of <code>ZwMapViewOfSection</code>, you can see the hook is gone.</p>
<p><img src="/classic-api-unhooking/s4.png" alt=""></p>
<h2 id="afterword">Afterword</h2>
<p>First of all, as a simple sample, I did not put any effort in obfuscating my function calls/ strings etc. So the code is definitely far from ready-to-go.<br>
There will probably be posts about basic obfuscation in the future.</p>
<p>And, is it a perfect solution for API unhooking? Of course not, as it&rsquo;s called <strong>Classic</strong> unhooking :)<br>
In fact, there is a part looks suspicious when we unhooking like this, as in the EDR&rsquo;s point of view:</p>
<blockquote>
<p>Why the heck is this process reading <code>ntdll.dll</code> from disk???<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></p>
</blockquote>
<p>Yes, normal process should not read <code>ntdll.dll</code> &ldquo;manually&rdquo;, as it should be automatically loaded. There will be posts about more advanced technique for unhooking in the future.</p>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p><a href="https://www.cyberbit.com/blog/endpoint-security/malware-mitigation-when-direct-system-calls-are-used/">https://www.cyberbit.com/blog/endpoint-security/malware-mitigation-when-direct-system-calls-are-used/</a> <a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>
]]></content></item></channel></rss>